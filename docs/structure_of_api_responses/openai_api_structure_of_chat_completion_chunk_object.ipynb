{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This documentation is inferred from OpenAI's official documentation as of 2025-01-01\n",
    "\n",
    "\n",
    "def ModelResponse():\n",
    "    return (\n",
    "        \"This object returned from here (in case `stream=True`): \"\n",
    "        + \"https://platform.openai.com/docs/api-reference/chat/streaming\"\n",
    "    )\n",
    "\n",
    "\n",
    "def StreamingChoices():\n",
    "    return (\n",
    "        \"This object returned under `choices` as mentioned here: \"\n",
    "        + \"https://platform.openai.com/docs/api-reference/chat/streaming#chat/streaming-choices\"\n",
    "    )\n",
    "\n",
    "\n",
    "def Delta():\n",
    "    return (\n",
    "        \"This object returned under `delta` as mentioned under `choices` here: \"\n",
    "        + \"https://platform.openai.com/docs/api-reference/chat/streaming#chat/streaming-choices\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ChatCompletionDeltaToolCall():\n",
    "    return (\n",
    "        \"This object returned under `tool_calls` as mentioned in this abstract implementation: \"\n",
    "        + \"https://github.com/openai/openai-python/blob/main/src/openai/types/chat/chat_completion_message_tool_call.py\"\n",
    "    )\n",
    "\n",
    "\n",
    "def Function():\n",
    "    return (\n",
    "        \"This object returned under `ChatCompletionDeltaToolCall` object as mentioned in this abstract implementation: \"\n",
    "        + \"https://github.com/openai/openai-python/blob/main/src/openai/types/chat/chat_completion_message_tool_call.py\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelResponse(\n",
    "    id=\"Unique identifier for the response.\",\n",
    "    choices=[\n",
    "        StreamingChoices(\n",
    "            finish_reason=(\n",
    "                \"The reason the model stopped generating tokens. This will be `stop` if model hits a natural stop point or a \"\n",
    "                + \"provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, \"\n",
    "                + \"`content_filter` if content was omitted due to a flag from our content filters,\"\n",
    "                + \"`tool_calls` if the model called a tool, \"\n",
    "                + \"or function_call (deprecated) if the model called a function.\"\n",
    "            ),\n",
    "            index=\"The index of the choice in the list of choices.\",\n",
    "            delta=Delta(\n",
    "                refusal=\"Reason for refusal, if any.\",\n",
    "                content=(\n",
    "                    \"The contents of the chunk message.\"\n",
    "                    + \"SUBTLE NOTE: Will be `None` if the value of the `tool_calls` key is not an empty list `[]`.\"\n",
    "                ),\n",
    "                role=\"The role of the author of this message chunk (user, role, or assistant).\",\n",
    "                function_call=(\n",
    "                    \"###### Deprecated ###### and replaced by `tool_calls`.\"\n",
    "                    + \"The name and arguments of a function that should be called, as generated by the model.\"\n",
    "                ),\n",
    "                # SUBTLE NOTE: `tool_calls` value will be an empty list if the model deduced that no tool calls are needed\n",
    "                tool_calls=[\n",
    "                    ChatCompletionDeltaToolCall(\n",
    "                        id=\"The ID of the tool call.\",\n",
    "                        type=\"The type of the tool. As of 2024-09-01, only `function` is supported.\",\n",
    "                        function=Function(\n",
    "                            name=\"Name of the function being called\",\n",
    "                            arguments=(\n",
    "                                \"The arguments to call the function with, as generated by the model in JSON format.\"\n",
    "                                + \"Note that the model does not always generate valid JSON, \"\n",
    "                                + \"and may hallucinate parameters not defined by your function schema.\"\n",
    "                                + \"Validate the arguments in your code before calling your function.\"\n",
    "                            ),\n",
    "                        ),\n",
    "                        index=\"Index of the tool call in the response.\",\n",
    "                    )\n",
    "                ],\n",
    "            ),\n",
    "            logprobs=\"Log probabilities of the tokens, if available.\",\n",
    "        )\n",
    "    ],\n",
    "    created=\"Timestamp when the response was created.\",\n",
    "    model=\"Name of the model used to generate the response.\",\n",
    "    object=\"Type of the object (e.g., chat.completion.chunk).\",\n",
    "    system_fingerprint=\"Unique fingerprint of the system.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
